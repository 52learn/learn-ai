{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` 内容来自B站视频内容 《B站首推！2025最新版大模型RAG入门到精通实战教程！手把手带你结合企业级项目实战完成一套完整的RAG项目！增加检索/文本向量/知识库搭建》```\n",
    "# 文档的加载与切割 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 pdf 解析库\n",
    "!pip install pdfminer.six "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename,page_numbers=None,min_line_length=1):\n",
    "    ''' 从PDF文件中(指定页码) 提取文字 '''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页码\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith(' ') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\",min_line_length=10)\n",
    "for para in paragraphs[:4]:\n",
    "    print(para + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM接口封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(),verbose=True) \n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(promt, model=\"gpt-3.5-turbo\"):\n",
    "    ''' 封装 openAI 接口 '''\n",
    "    messages = [{\"role\": \"user\", \"content\": promt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # 模型输出结果的随机性，0-1之间，值越大，输出结果越随机\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建Prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(promt_template, **kwargs):\n",
    "    ''' 将prompt模板和参数拼接起来 '''\n",
    "    inputs = {}\n",
    "    for k,v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(i, str) for i in v):\n",
    "            val = '\\n\\n'.join(v)\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return promt_template.format(**inputs)  \n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "已知信息：\n",
    "{context}\n",
    "用户问：\n",
    "{query}\n",
    "\n",
    "如果已知信息不包含用户问题的答案，请直接说\"不知道\"。\n",
    "请用中文回答。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量间的相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def l2(a,b):\n",
    "    x = np.asarray(a)-np.asarray(b)\n",
    "    return norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转换为向量\n",
    "def get_embeddings(texts,model='text-embedding-ada-002',dimensions=None):\n",
    "    '''封装OpenAI的embedding模型接口，返回embedding列表和embedding向量维度'''\n",
    "    if model == 'text-embedding-ada-002':\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(input=texts,model=model,dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts,model=model).data\n",
    "    print(data)\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query=[\"测试文本\"]\n",
    "vec = get_embeddings(test_query)[0]\n",
    "print(f\"Total dimension: {len(vec)}\")\n",
    "print(f\"First 10 dimensions: {vec[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"国际争端\"\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings(query)[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Cosine distance:\") #越大越相似\n",
    "print(cos_sim(query_vec,query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec,vec))\n",
    "\n",
    "print(\"\\nEuclidean distance:\") #越小越相似\n",
    "print(l2(query_vec,query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec,vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chroma 向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\",page_numbers=[2,3],min_line_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self,collection_name,embedding_fn):\n",
    "        #内存模式\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "        #数据持久化\n",
    "        #chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        # 清空数据库\n",
    "        chroma_client.reset()\n",
    "        self.collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "    def add_documents(self,documents):\n",
    "        ''' 向 collection 中添加 documents '''\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))],\n",
    "            embeddings= self.embedding_fn(documents)\n",
    "        )\n",
    "    def search(self,query,top_n):\n",
    "        ''' 检索向量数据库 '''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )   \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = MyVectorDBConnector(\"demo\",get_embeddings)\n",
    "vector_db.add_documents(paragraphs)\n",
    "user_query = \"Llama 2有多少个参数\"\n",
    "results = vector_db.search(user_query,2)\n",
    "for para in results['documents'][0]:\n",
    "    print(para,\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于向量检索的RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self,vector_db,llm_api,n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "    def chat(self,user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query,self.n_results)\n",
    "        # 2. 构建Promt\n",
    "        prompt = build_prompt(prompt_template,context=search_results['documents'][0],query=user_query)\n",
    "        # 3. 调用LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(vector_db,llm_api=get_completion)\n",
    "user_query = \"Llama 2有多少个参数\"\n",
    "response = bot.chat(user_query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
